{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to Deep Learning and `keras`\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock.    \n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
    "\n",
    "**Data Set Information:**\n",
    "\n",
    "The file \"sonar.mines\" contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file \"sonar.rocks\" contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock. \n",
    "\n",
    "Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp. \n",
    "\n",
    "The label associated with each record contains the letter \"R\" if the object is a rock and \"M\" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset\n",
    "data = pd.read_csv(\"./data/sonar.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the first 5 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find number of rows and columns in data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find count of R and M in the target \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "data = data.values\n",
    "X = data[:,0:60].astype(float)\n",
    "y = data[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import keras and scikit-learn libraries.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, encoded_y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: If the target variable is \n",
    "#distributed similarly in both train and test.\n",
    "#If you think it isn't, you could use stratified sampling.\n",
    "#For this notebook, we will stick to what we have gotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([84, 82]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([27, 15]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 5\n",
    "verbose = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 0.6929 - acc: 0.4940\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 0s 313us/step - loss: 0.6913 - acc: 0.5783\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 0s 323us/step - loss: 0.6902 - acc: 0.5964\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 0s 345us/step - loss: 0.6893 - acc: 0.5843\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 0s 310us/step - loss: 0.6886 - acc: 0.5602\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 0s 312us/step - loss: 0.6879 - acc: 0.5964\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 0s 278us/step - loss: 0.6872 - acc: 0.5843\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 0s 293us/step - loss: 0.6863 - acc: 0.5663\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - 0s 294us/step - loss: 0.6857 - acc: 0.6205\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - 0s 301us/step - loss: 0.6846 - acc: 0.6145\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - 0s 400us/step - loss: 0.6839 - acc: 0.6566\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - 0s 344us/step - loss: 0.6831 - acc: 0.6446\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - 0s 442us/step - loss: 0.6823 - acc: 0.6446\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - 0s 276us/step - loss: 0.6816 - acc: 0.6566\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - 0s 301us/step - loss: 0.6802 - acc: 0.6506\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - 0s 287us/step - loss: 0.6793 - acc: 0.6386\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - 0s 284us/step - loss: 0.6786 - acc: 0.6687\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - 0s 250us/step - loss: 0.6772 - acc: 0.6446\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - 0s 243us/step - loss: 0.6760 - acc: 0.6386\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - 0s 217us/step - loss: 0.6749 - acc: 0.6506\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - 0s 215us/step - loss: 0.6734 - acc: 0.6566\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - 0s 230us/step - loss: 0.6719 - acc: 0.6506\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.6712 - acc: 0.6446\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - 0s 242us/step - loss: 0.6696 - acc: 0.6627\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - 0s 233us/step - loss: 0.6678 - acc: 0.6687\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - 0s 232us/step - loss: 0.6662 - acc: 0.6687\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - 0s 267us/step - loss: 0.6642 - acc: 0.6747\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - 0s 303us/step - loss: 0.6624 - acc: 0.6867\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - 0s 273us/step - loss: 0.6607 - acc: 0.6747\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - 0s 299us/step - loss: 0.6587 - acc: 0.6807\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - 0s 281us/step - loss: 0.6571 - acc: 0.6747\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - 0s 287us/step - loss: 0.6545 - acc: 0.7048\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - 0s 334us/step - loss: 0.6522 - acc: 0.6928\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - 0s 301us/step - loss: 0.6505 - acc: 0.6928\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - 0s 315us/step - loss: 0.6477 - acc: 0.6807\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - 0s 310us/step - loss: 0.6452 - acc: 0.6928\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - 0s 282us/step - loss: 0.6428 - acc: 0.6747\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - 0s 283us/step - loss: 0.6401 - acc: 0.7289\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - 0s 285us/step - loss: 0.6385 - acc: 0.6928\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - 0s 260us/step - loss: 0.6356 - acc: 0.6928\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - 0s 264us/step - loss: 0.6320 - acc: 0.7289\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - 0s 241us/step - loss: 0.6303 - acc: 0.6928\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - 0s 257us/step - loss: 0.6270 - acc: 0.7289\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - 0s 241us/step - loss: 0.6241 - acc: 0.7108\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - 0s 244us/step - loss: 0.6206 - acc: 0.7169\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - 0s 288us/step - loss: 0.6175 - acc: 0.7349\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - 0s 277us/step - loss: 0.6153 - acc: 0.7349\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - 0s 282us/step - loss: 0.6116 - acc: 0.7108\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - 0s 258us/step - loss: 0.6075 - acc: 0.7470\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - 0s 255us/step - loss: 0.6046 - acc: 0.7349\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - 0s 290us/step - loss: 0.6010 - acc: 0.7470\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - 0s 244us/step - loss: 0.5967 - acc: 0.7470\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - 0s 274us/step - loss: 0.5939 - acc: 0.7289\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - 0s 272us/step - loss: 0.5899 - acc: 0.7229\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - 0s 271us/step - loss: 0.5874 - acc: 0.7590\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - 0s 275us/step - loss: 0.5851 - acc: 0.7289\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - 0s 282us/step - loss: 0.5804 - acc: 0.7530\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - 0s 293us/step - loss: 0.5764 - acc: 0.7530\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - 0s 279us/step - loss: 0.5735 - acc: 0.7530\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - 0s 263us/step - loss: 0.5688 - acc: 0.7651\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - 0s 271us/step - loss: 0.5654 - acc: 0.7651\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - 0s 258us/step - loss: 0.5606 - acc: 0.7530\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - 0s 276us/step - loss: 0.5571 - acc: 0.7831\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - 0s 235us/step - loss: 0.5546 - acc: 0.7831\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - 0s 234us/step - loss: 0.5510 - acc: 0.7892\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - 0s 225us/step - loss: 0.5480 - acc: 0.7831\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - 0s 207us/step - loss: 0.5438 - acc: 0.7771\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - 0s 302us/step - loss: 0.5401 - acc: 0.7892\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - 0s 281us/step - loss: 0.5362 - acc: 0.7892\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - 0s 243us/step - loss: 0.5320 - acc: 0.7892\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - 0s 215us/step - loss: 0.5278 - acc: 0.7711\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - 0s 269us/step - loss: 0.5258 - acc: 0.7892\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - 0s 253us/step - loss: 0.5208 - acc: 0.7892\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - 0s 257us/step - loss: 0.5201 - acc: 0.8012\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - 0s 229us/step - loss: 0.5165 - acc: 0.7892\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - 0s 227us/step - loss: 0.5119 - acc: 0.7892\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - 0s 226us/step - loss: 0.5077 - acc: 0.7771\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - 0s 421us/step - loss: 0.5066 - acc: 0.7831\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - 0s 239us/step - loss: 0.5019 - acc: 0.7771\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - 0s 256us/step - loss: 0.5029 - acc: 0.7831\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - 0s 217us/step - loss: 0.4959 - acc: 0.7711\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - 0s 225us/step - loss: 0.4952 - acc: 0.7892\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 216us/step - loss: 0.4902 - acc: 0.7952\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - 0s 240us/step - loss: 0.4876 - acc: 0.7952\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.4850 - acc: 0.7952\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - 0s 240us/step - loss: 0.4825 - acc: 0.8012\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - 0s 247us/step - loss: 0.4824 - acc: 0.7952\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.4756 - acc: 0.7892\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - 0s 204us/step - loss: 0.4756 - acc: 0.8012\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - 0s 225us/step - loss: 0.4722 - acc: 0.7892\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - 0s 201us/step - loss: 0.4711 - acc: 0.7831\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - 0s 244us/step - loss: 0.4677 - acc: 0.7952\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - 0s 235us/step - loss: 0.4654 - acc: 0.7771\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - 0s 241us/step - loss: 0.4614 - acc: 0.7892\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - 0s 261us/step - loss: 0.4607 - acc: 0.7952\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - 0s 237us/step - loss: 0.4592 - acc: 0.7952\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - 0s 248us/step - loss: 0.4562 - acc: 0.7892\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.4552 - acc: 0.8012\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - 0s 265us/step - loss: 0.4519 - acc: 0.8133\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - 0s 264us/step - loss: 0.4509 - acc: 0.7892\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f27929c3748>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=nb_epoch, \n",
    "          batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5033402889966965, 0.7380952537059784]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the below code \n",
    "# - add another dense layer - with 30 neurons. \n",
    "# - Set the activation to sigmoid\n",
    "# - Observe what happens if you set the activation to relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 0.6986 - acc: 0.5060\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 0s 286us/step - loss: 0.6926 - acc: 0.5301\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 0s 287us/step - loss: 0.6894 - acc: 0.5060\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 0s 270us/step - loss: 0.6870 - acc: 0.6145\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 0s 294us/step - loss: 0.6858 - acc: 0.5060\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 0s 303us/step - loss: 0.6813 - acc: 0.5964\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 0s 296us/step - loss: 0.6772 - acc: 0.7048\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 0s 304us/step - loss: 0.6729 - acc: 0.5301\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - 0s 293us/step - loss: 0.6692 - acc: 0.6807\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - 0s 304us/step - loss: 0.6642 - acc: 0.7048\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - 0s 261us/step - loss: 0.6580 - acc: 0.7048\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - 0s 305us/step - loss: 0.6520 - acc: 0.6928\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - 0s 540us/step - loss: 0.6458 - acc: 0.7470\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - 0s 475us/step - loss: 0.6413 - acc: 0.7289\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - 0s 367us/step - loss: 0.6297 - acc: 0.7892\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - 0s 270us/step - loss: 0.6223 - acc: 0.7831\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - 0s 448us/step - loss: 0.6158 - acc: 0.7831\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - 0s 404us/step - loss: 0.6081 - acc: 0.7470\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - 0s 420us/step - loss: 0.5984 - acc: 0.7169\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - 0s 298us/step - loss: 0.5894 - acc: 0.7952\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.5778 - acc: 0.8012\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - 0s 258us/step - loss: 0.5734 - acc: 0.7651\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - 0s 291us/step - loss: 0.5628 - acc: 0.7892\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - 0s 405us/step - loss: 0.5527 - acc: 0.8133\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - 0s 429us/step - loss: 0.5483 - acc: 0.7831\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - 0s 418us/step - loss: 0.5372 - acc: 0.8253\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - 0s 268us/step - loss: 0.5289 - acc: 0.8193\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - 0s 281us/step - loss: 0.5254 - acc: 0.8072\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.5128 - acc: 0.8313\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - 0s 276us/step - loss: 0.5097 - acc: 0.8133\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - 0s 247us/step - loss: 0.5010 - acc: 0.8133\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - 0s 266us/step - loss: 0.4962 - acc: 0.8133\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - 0s 343us/step - loss: 0.4960 - acc: 0.7892\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - 0s 492us/step - loss: 0.4898 - acc: 0.7952\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - 0s 442us/step - loss: 0.4781 - acc: 0.8373\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - 0s 262us/step - loss: 0.4722 - acc: 0.8313\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - 0s 257us/step - loss: 0.4693 - acc: 0.8193\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - 0s 261us/step - loss: 0.4604 - acc: 0.8434\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - 0s 256us/step - loss: 0.4619 - acc: 0.8253\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - 0s 353us/step - loss: 0.4534 - acc: 0.8434\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - 0s 588us/step - loss: 0.4503 - acc: 0.8554\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - 0s 465us/step - loss: 0.4447 - acc: 0.8434\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - 0s 238us/step - loss: 0.4413 - acc: 0.8373\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - 0s 239us/step - loss: 0.4380 - acc: 0.8313\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - 0s 234us/step - loss: 0.4372 - acc: 0.8072\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - 0s 258us/step - loss: 0.4325 - acc: 0.8253\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - 0s 277us/step - loss: 0.4297 - acc: 0.8313\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - 0s 256us/step - loss: 0.4277 - acc: 0.8133\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.4213 - acc: 0.8313\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - 0s 266us/step - loss: 0.4196 - acc: 0.8373\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - 0s 215us/step - loss: 0.4182 - acc: 0.8313\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - 0s 233us/step - loss: 0.4164 - acc: 0.8313\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - 0s 256us/step - loss: 0.4131 - acc: 0.8434\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - 0s 255us/step - loss: 0.4110 - acc: 0.8193\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - 0s 242us/step - loss: 0.4107 - acc: 0.8253\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - 0s 260us/step - loss: 0.4127 - acc: 0.8193\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.4099 - acc: 0.8253\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - 0s 306us/step - loss: 0.4129 - acc: 0.8193\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - 0s 293us/step - loss: 0.4039 - acc: 0.8494\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - 0s 290us/step - loss: 0.4056 - acc: 0.8253\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - 0s 279us/step - loss: 0.3994 - acc: 0.8313\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - 0s 299us/step - loss: 0.3962 - acc: 0.8253\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - 0s 296us/step - loss: 0.4084 - acc: 0.8253\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - 0s 308us/step - loss: 0.3979 - acc: 0.8253\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - 0s 306us/step - loss: 0.4048 - acc: 0.8072\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - 0s 291us/step - loss: 0.3918 - acc: 0.8313\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.3891 - acc: 0.8253\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - 0s 243us/step - loss: 0.3900 - acc: 0.8253\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - 0s 311us/step - loss: 0.3862 - acc: 0.8253\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - 0s 231us/step - loss: 0.3870 - acc: 0.8193\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - 0s 304us/step - loss: 0.3861 - acc: 0.8313\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - 0s 440us/step - loss: 0.3816 - acc: 0.8253\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - 0s 301us/step - loss: 0.3890 - acc: 0.8313\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - 0s 342us/step - loss: 0.3812 - acc: 0.8253\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - 0s 243us/step - loss: 0.3804 - acc: 0.8313\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - 0s 295us/step - loss: 0.3838 - acc: 0.8133\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - 0s 283us/step - loss: 0.3767 - acc: 0.8253\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - 0s 296us/step - loss: 0.3828 - acc: 0.8133\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - 0s 298us/step - loss: 0.3776 - acc: 0.8373\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - 0s 268us/step - loss: 0.3748 - acc: 0.8373\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - 0s 295us/step - loss: 0.3761 - acc: 0.8313\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - 0s 314us/step - loss: 0.3728 - acc: 0.8313\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 293us/step - loss: 0.3745 - acc: 0.8193\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - 0s 295us/step - loss: 0.3722 - acc: 0.8494\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - 0s 362us/step - loss: 0.3749 - acc: 0.8434\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - 0s 311us/step - loss: 0.3719 - acc: 0.8313\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - 0s 283us/step - loss: 0.3733 - acc: 0.8494\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - 0s 261us/step - loss: 0.3665 - acc: 0.8313\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - 0s 270us/step - loss: 0.3706 - acc: 0.8434\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - 0s 266us/step - loss: 0.3676 - acc: 0.8494\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - 0s 311us/step - loss: 0.3713 - acc: 0.8313\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - 0s 266us/step - loss: 0.3682 - acc: 0.8494\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - 0s 318us/step - loss: 0.3646 - acc: 0.8373\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - 0s 261us/step - loss: 0.3631 - acc: 0.8494\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - 0s 284us/step - loss: 0.3639 - acc: 0.8313\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - 0s 255us/step - loss: 0.3684 - acc: 0.8373\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - 0s 237us/step - loss: 0.3618 - acc: 0.8494\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - 0s 280us/step - loss: 0.3600 - acc: 0.8614\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - 0s 243us/step - loss: 0.3612 - acc: 0.8434\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.3629 - acc: 0.8494\n",
      "42/42 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#Modify this code ! Add one more layer with activation sigmoid\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='sigmoid'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=nb_epoch, \n",
    "          batch_size=batch_size, verbose=verbose)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.46106593913975213, 0.7619047725484485]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "166/166 [==============================] - 0s 2ms/step - loss: 0.6916 - acc: 0.5120\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 0s 514us/step - loss: 0.6817 - acc: 0.6205\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 0s 435us/step - loss: 0.6670 - acc: 0.7289\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 0s 444us/step - loss: 0.6514 - acc: 0.6747\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 0s 471us/step - loss: 0.6237 - acc: 0.7048\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 0s 438us/step - loss: 0.5961 - acc: 0.7831\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 0s 344us/step - loss: 0.5698 - acc: 0.7771\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.5509 - acc: 0.7169\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - 0s 247us/step - loss: 0.5236 - acc: 0.7349\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - 0s 237us/step - loss: 0.4988 - acc: 0.8253\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.4834 - acc: 0.8373\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - 0s 247us/step - loss: 0.4714 - acc: 0.7831\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.4720 - acc: 0.7470\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - 0s 250us/step - loss: 0.4553 - acc: 0.7892\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - 0s 265us/step - loss: 0.4443 - acc: 0.8193\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.4334 - acc: 0.8253\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - 0s 249us/step - loss: 0.4295 - acc: 0.8012\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - 0s 270us/step - loss: 0.4208 - acc: 0.8072\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.4085 - acc: 0.8313\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - 0s 264us/step - loss: 0.4122 - acc: 0.8133\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - 0s 260us/step - loss: 0.4157 - acc: 0.8012\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - 0s 236us/step - loss: 0.3948 - acc: 0.8193\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - 0s 237us/step - loss: 0.4173 - acc: 0.7892\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - 0s 228us/step - loss: 0.3996 - acc: 0.8313\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - 0s 238us/step - loss: 0.3888 - acc: 0.8193\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - 0s 249us/step - loss: 0.3800 - acc: 0.8373\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - 0s 249us/step - loss: 0.4034 - acc: 0.8133\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - 0s 245us/step - loss: 0.3795 - acc: 0.8434\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - 0s 234us/step - loss: 0.3736 - acc: 0.8193\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - 0s 259us/step - loss: 0.3725 - acc: 0.8313\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - 0s 235us/step - loss: 0.3654 - acc: 0.8313\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.3635 - acc: 0.8373\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - 0s 244us/step - loss: 0.3656 - acc: 0.8193\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - 0s 269us/step - loss: 0.3641 - acc: 0.8494\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.3609 - acc: 0.8253\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - 0s 255us/step - loss: 0.3581 - acc: 0.8373\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - 0s 268us/step - loss: 0.3578 - acc: 0.8373\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.3516 - acc: 0.8253\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - 0s 293us/step - loss: 0.3742 - acc: 0.8133\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - 0s 256us/step - loss: 0.3553 - acc: 0.8494\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - 0s 266us/step - loss: 0.3490 - acc: 0.8373\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - 0s 236us/step - loss: 0.3567 - acc: 0.8434\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - 0s 242us/step - loss: 0.3381 - acc: 0.8614\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - 0s 519us/step - loss: 0.3405 - acc: 0.8494\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - 0s 472us/step - loss: 0.3371 - acc: 0.8614\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - 0s 427us/step - loss: 0.3398 - acc: 0.8494\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - 0s 253us/step - loss: 0.3319 - acc: 0.8434\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - 0s 262us/step - loss: 0.3301 - acc: 0.8614\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - 0s 285us/step - loss: 0.3353 - acc: 0.8614\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - 0s 258us/step - loss: 0.3187 - acc: 0.8554\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - 0s 265us/step - loss: 0.3338 - acc: 0.8434\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - 0s 241us/step - loss: 0.3201 - acc: 0.8554\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - 0s 256us/step - loss: 0.3189 - acc: 0.8675\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - 0s 236us/step - loss: 0.3160 - acc: 0.8675\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - 0s 244us/step - loss: 0.3234 - acc: 0.8494\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - 0s 246us/step - loss: 0.3270 - acc: 0.8313\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.3175 - acc: 0.8614\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - 0s 264us/step - loss: 0.3052 - acc: 0.8795\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - 0s 241us/step - loss: 0.3222 - acc: 0.8253\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.3030 - acc: 0.8976\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - 0s 241us/step - loss: 0.3081 - acc: 0.8494\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - 0s 251us/step - loss: 0.2965 - acc: 0.8795\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - 0s 252us/step - loss: 0.2989 - acc: 0.8675\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - 0s 229us/step - loss: 0.3031 - acc: 0.8614\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - 0s 250us/step - loss: 0.2907 - acc: 0.8855\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - 0s 253us/step - loss: 0.3110 - acc: 0.8494\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - 0s 269us/step - loss: 0.2863 - acc: 0.8614\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - 0s 321us/step - loss: 0.3009 - acc: 0.8735\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - 0s 287us/step - loss: 0.2887 - acc: 0.8675\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - 0s 283us/step - loss: 0.2820 - acc: 0.8976\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - 0s 318us/step - loss: 0.2871 - acc: 0.8675\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - 0s 265us/step - loss: 0.3048 - acc: 0.8494\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - 0s 303us/step - loss: 0.2737 - acc: 0.9036\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - 0s 325us/step - loss: 0.2720 - acc: 0.8795\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - 0s 329us/step - loss: 0.2718 - acc: 0.8735\n",
      "Epoch 76/100\n",
      "166/166 [==============================] - 0s 324us/step - loss: 0.2876 - acc: 0.8976\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - 0s 375us/step - loss: 0.2774 - acc: 0.8735\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - 0s 343us/step - loss: 0.2721 - acc: 0.8675\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - 0s 259us/step - loss: 0.2773 - acc: 0.8976\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - 0s 300us/step - loss: 0.2604 - acc: 0.8855\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - 0s 253us/step - loss: 0.2612 - acc: 0.8976\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - 0s 266us/step - loss: 0.2603 - acc: 0.8976\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 295us/step - loss: 0.2613 - acc: 0.8795\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - 0s 262us/step - loss: 0.2558 - acc: 0.8855\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - 0s 263us/step - loss: 0.2526 - acc: 0.8795\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - 0s 321us/step - loss: 0.2613 - acc: 0.9036\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - 0s 263us/step - loss: 0.2556 - acc: 0.8916\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - 0s 282us/step - loss: 0.2474 - acc: 0.9157\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - 0s 310us/step - loss: 0.2438 - acc: 0.9036\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - 0s 282us/step - loss: 0.2692 - acc: 0.8735\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - 0s 253us/step - loss: 0.2402 - acc: 0.8916\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.2439 - acc: 0.9036\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - 0s 264us/step - loss: 0.2612 - acc: 0.8735\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - 0s 296us/step - loss: 0.2364 - acc: 0.9157\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - 0s 283us/step - loss: 0.2398 - acc: 0.9217\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - 0s 284us/step - loss: 0.2348 - acc: 0.8916\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - 0s 296us/step - loss: 0.2296 - acc: 0.9217\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - 0s 247us/step - loss: 0.2262 - acc: 0.9157\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - 0s 254us/step - loss: 0.2251 - acc: 0.9096\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - 0s 267us/step - loss: 0.2270 - acc: 0.9096\n",
      "42/42 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#Modify this code ! Add one more layer with activation relu\n",
    "model = Sequential()\n",
    "model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=nb_epoch, \n",
    "          batch_size=batch_size, verbose=verbose)\n",
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4505758388411431, 0.8333333432674408]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
